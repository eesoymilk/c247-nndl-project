optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
lr_scheduler:
  scheduler:
    _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
    warmup_epochs: 10
    max_epochs: 100
    warmup_start_lr: 1.0e-08
    eta_min: 1.0e-06
  interval: epoch
lstm_layers: 2
lstm_hidden_size: 192
gru_layers: 1
gru_hidden_size: 64
lstm_dropout: 0.3
between_dropout: 0.3
gru_dropout: 0.0
gru_bidirectional: true
