\section{Discussion}

\subsubsection{Model Architecture}

A critical context for our experimental results is the use of data from a single user, restricting both data volume and variability. Among the evaluated models, the baseline TDSConv architecture consistently achieved superior performance, demonstrating robust decoding capability for keystrokes from sEMG signals.

The purely convolutional TCN model initially showed rapid convergence but quickly plateaued, exhibiting high validation and test losses. This behavior suggests the model’s limited ability to capture long-term dependencies essential in sEMG data, and its tendency toward overfitting given the limited dataset size.

Conversely, the purely recurrent LSTM-GRU model exhibited slower yet continuous improvement in performance, highlighting its effectiveness in modeling temporal dependencies inherent in sEMG signals. The hybrid model (TCN+LSTM+GRU) similarly leveraged recurrent components to capture temporal relationships; however, the added complexity from convolutional layers prolonged convergence times and increased vulnerability to overfitting.

Overall, these findings suggest that recurrent models are better suited for capturing temporal dynamics in sEMG signals, especially when dataset diversity and volume are constrained. Future research should consider multi-user datasets, longer training periods, and advanced regularization or data augmentation methods to fully exploit the potential of hybrid and recurrent architectures.

\subsubsection{Preprocessing Techniques}
The results of our experiments showed that the model using the 50 Hz notch filter achieved a validation CER that was 8.2\% lower than the baseline model at Epoch 50. This may indicate that removing power line interference is crucial for processing sEMG signals. After removing the 50 Hz interference, the model can focus more on the effective frequency components in the signal, thereby extracting more meaningful features and improving the final performance.

Adaptive Gaussian noise and z-score normalization did not significantly surpass the baseline in the final CER (22.13 and 22.37, respectively, close to the baseline's 20.98), but they performed significantly better than the baseline model in the early stages of training. This may be because normalization and noise addition reduce the model's sensitivity to the scale and distribution of the input data in the early stages, allowing the network to learn the main patterns in the signal faster, thus accelerating convergence. However, in the later stages, the advantages of these preprocessing methods may be balanced by other factors, resulting in final performance that is close to the baseline.

When two bandpass filters with different cutoff frequencies (20 Hz–500 Hz and 20 Hz–150 Hz) were used, the model had very high CER at the beginning of training and poor overall performance. This may indicate that the bandpass filter failed to retain enough of the effective signal components, or there were side effects in the filter design that resulted in the loss of useful information. The much lower CER of these models can be attributed to the bandpass filters suppressing noise, but they may have also weakened some important detailed features that help the model learn. The hyperparameters of the bandpass filter for this task need further optimization to achieve better performance.

From the results, it is evident that the preprocessing techniques can help the model converge faster to some extent. This also suggests that appropriate preprocessing techniques can play an important role in improving learning efficiency.